MDSTopicKVS(
  (enc_word_embed): Embedding(32000, 256, padding_idx=6)
  (enc_embed_dropout): Dropout(p=0.1, inplace=False)
  (dec_embed): Embedding(32000, 256, padding_idx=6)
  (dec_embed_dropout): Dropout(p=0.1, inplace=False)
  (enc_pos_embed): PositionalEncoding()
  (dec_pos_embed): PositionalEncoding()
  (topic_embed): Embedding(32000, 256, padding_idx=6)
  (transformer_encoder): TransformerEncoder(
    (transformer_encoder_layers): ModuleList(
      (0): TransformerEncoderLayer(
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (1): TransformerEncoderLayer(
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (2): TransformerEncoderLayer(
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (3): TransformerEncoderLayer(
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (4): TransformerEncoderLayer(
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (5): TransformerEncoderLayer(
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (graph_encoder): GraphEncoder(
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
    (self_attn_pooling_layer): SelfAttentionPoolingLayer(
      (multi_head_pooling): MultiHeadPooling(
        (w_ks): Linear(in_features=256, out_features=8, bias=True)
        (w_vs): Linear(in_features=256, out_features=256, bias=True)
        (fc): Linear(in_features=256, out_features=256, bias=True)
        (attn): DotProductPooling(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (graph_encoder_layers): ModuleList(
      (0): GraphEncoderLayer(
        (multi_head_structure_attn): MultiHeadStructureAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (graph_attn): GraphScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (1): GraphEncoderLayer(
        (multi_head_structure_attn): MultiHeadStructureAttention(
          (dropout): Dropout(p=0.1, inplace=False)
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (graph_attn): GraphScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout): Dropout(p=0.1, inplace=False)
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
  (enc_layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  (graph_decoder): GraphDecoder(
    (graph_decoder_layers): ModuleList(
      (0): GraphDecoderLayer(
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_hierarchical_attn): MultiHeadHierarchicalAttention(
          (w_qs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_s): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_w): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_t): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_t): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_t): Linear(in_features=256, out_features=256, bias=True)
          (fc_topic): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=768, out_features=256, bias=True)
          (graph_attn): GraphScaledDotProductAttentionWithMask(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc_pos_v): Linear(in_features=32, out_features=32, bias=True)
            (fc_pos_s): Linear(in_features=32, out_features=1, bias=True)
            (fc_out): Linear(in_features=256, out_features=256, bias=True)
          )
          (topic_attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (attn_with_sent_norm): ScaledDotProductAttentionWithSentenceNorm(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (1): GraphDecoderLayer(
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_hierarchical_attn): MultiHeadHierarchicalAttention(
          (w_qs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_s): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_w): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_t): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_t): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_t): Linear(in_features=256, out_features=256, bias=True)
          (fc_topic): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=768, out_features=256, bias=True)
          (graph_attn): GraphScaledDotProductAttentionWithMask(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc_pos_v): Linear(in_features=32, out_features=32, bias=True)
            (fc_pos_s): Linear(in_features=32, out_features=1, bias=True)
            (fc_out): Linear(in_features=256, out_features=256, bias=True)
          )
          (topic_attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (attn_with_sent_norm): ScaledDotProductAttentionWithSentenceNorm(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (2): GraphDecoderLayer(
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_hierarchical_attn): MultiHeadHierarchicalAttention(
          (w_qs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_s): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_w): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_t): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_t): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_t): Linear(in_features=256, out_features=256, bias=True)
          (fc_topic): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=768, out_features=256, bias=True)
          (graph_attn): GraphScaledDotProductAttentionWithMask(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc_pos_v): Linear(in_features=32, out_features=32, bias=True)
            (fc_pos_s): Linear(in_features=32, out_features=1, bias=True)
            (fc_out): Linear(in_features=256, out_features=256, bias=True)
          )
          (topic_attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (attn_with_sent_norm): ScaledDotProductAttentionWithSentenceNorm(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (3): GraphDecoderLayer(
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_hierarchical_attn): MultiHeadHierarchicalAttention(
          (w_qs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_s): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_w): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_t): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_t): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_t): Linear(in_features=256, out_features=256, bias=True)
          (fc_topic): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=768, out_features=256, bias=True)
          (graph_attn): GraphScaledDotProductAttentionWithMask(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc_pos_v): Linear(in_features=32, out_features=32, bias=True)
            (fc_pos_s): Linear(in_features=32, out_features=1, bias=True)
            (fc_out): Linear(in_features=256, out_features=256, bias=True)
          )
          (topic_attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (attn_with_sent_norm): ScaledDotProductAttentionWithSentenceNorm(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (4): GraphDecoderLayer(
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_hierarchical_attn): MultiHeadHierarchicalAttention(
          (w_qs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_s): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_w): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_t): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_t): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_t): Linear(in_features=256, out_features=256, bias=True)
          (fc_topic): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=768, out_features=256, bias=True)
          (graph_attn): GraphScaledDotProductAttentionWithMask(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc_pos_v): Linear(in_features=32, out_features=32, bias=True)
            (fc_pos_s): Linear(in_features=32, out_features=1, bias=True)
            (fc_out): Linear(in_features=256, out_features=256, bias=True)
          )
          (topic_attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (attn_with_sent_norm): ScaledDotProductAttentionWithSentenceNorm(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (5): GraphDecoderLayer(
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_hierarchical_attn): MultiHeadHierarchicalAttention(
          (w_qs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_s): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_w): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_t): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_t): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_t): Linear(in_features=256, out_features=256, bias=True)
          (fc_topic): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=768, out_features=256, bias=True)
          (graph_attn): GraphScaledDotProductAttentionWithMask(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc_pos_v): Linear(in_features=32, out_features=32, bias=True)
            (fc_pos_s): Linear(in_features=32, out_features=1, bias=True)
            (fc_out): Linear(in_features=256, out_features=256, bias=True)
          )
          (topic_attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (attn_with_sent_norm): ScaledDotProductAttentionWithSentenceNorm(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (6): GraphDecoderLayer(
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_hierarchical_attn): MultiHeadHierarchicalAttention(
          (w_qs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_s): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_w): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_t): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_t): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_t): Linear(in_features=256, out_features=256, bias=True)
          (fc_topic): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=768, out_features=256, bias=True)
          (graph_attn): GraphScaledDotProductAttentionWithMask(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc_pos_v): Linear(in_features=32, out_features=32, bias=True)
            (fc_pos_s): Linear(in_features=32, out_features=1, bias=True)
            (fc_out): Linear(in_features=256, out_features=256, bias=True)
          )
          (topic_attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (attn_with_sent_norm): ScaledDotProductAttentionWithSentenceNorm(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
      (7): GraphDecoderLayer(
        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
        (dropout1): Dropout(p=0.1, inplace=False)
        (dropout2): Dropout(p=0.1, inplace=False)
        (self_attn): MultiHeadAttention(
          (w_qs): Linear(in_features=256, out_features=256, bias=True)
          (w_ks): Linear(in_features=256, out_features=256, bias=True)
          (w_vs): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=256, out_features=256, bias=True)
          (attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
        (multi_head_hierarchical_attn): MultiHeadHierarchicalAttention(
          (w_qs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_s): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_s): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_w): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_w): Linear(in_features=256, out_features=256, bias=True)
          (w_qs_t): Linear(in_features=256, out_features=256, bias=True)
          (w_ks_t): Linear(in_features=256, out_features=256, bias=True)
          (w_vs_t): Linear(in_features=256, out_features=256, bias=True)
          (fc_topic): Linear(in_features=256, out_features=256, bias=True)
          (fc): Linear(in_features=768, out_features=256, bias=True)
          (graph_attn): GraphScaledDotProductAttentionWithMask(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc_pos_v): Linear(in_features=32, out_features=32, bias=True)
            (fc_pos_s): Linear(in_features=32, out_features=1, bias=True)
            (fc_out): Linear(in_features=256, out_features=256, bias=True)
          )
          (topic_attn): ScaledDotProductAttention(
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (attn_with_sent_norm): ScaledDotProductAttentionWithSentenceNorm(
            (dropout): Dropout(p=0.1, inplace=False)
            (fc): Linear(in_features=256, out_features=256, bias=True)
          )
        )
        (pos_ffd): PositionwiseFeedForward(
          (fc1): Linear(in_features=256, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=256, bias=True)
          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
          (dropout_1): Dropout(p=0.1, inplace=False)
          (dropout_2): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)
  )
  (generator_fc): Linear(in_features=256, out_features=32000, bias=True)
  (generator_log_softmax): LogSoftmax(dim=-1)
)